{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7488486d",
   "metadata": {},
   "source": [
    "# CPU or GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5f13c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# # Configuración de TensorFlow para usar la GPU\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         for gpu in gpus:\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     except RuntimeError as e:\n",
    "#         print(e)\n",
    "\n",
    "# Configuración de TensorFlow para usar la CPU\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b422cb8",
   "metadata": {},
   "source": [
    "# LIBRARIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c130517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library EEG\n",
    "import mne\n",
    "\n",
    "# Library System \n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Library Statistics\n",
    "import scipy.stats\n",
    "import random\n",
    "from scipy.stats import entropy\n",
    "from scipy.fft import fft\n",
    "from scipy.signal import welch\n",
    "\n",
    "# Library GAN\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU, BatchNormalization, Reshape, Conv2DTranspose, Conv2D, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Library Graph\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# No Warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "mne.set_log_level(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597da088",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f0445d",
   "metadata": {},
   "source": [
    "# STATISTICS CALCULATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb0a9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo de ejecución del Código\n",
    "start_time = time.time()\n",
    "\n",
    "emotion='POSITIVE' # 'NEGATIVE' 'NEUTRAL'\n",
    "emotion_folder =f'C://Users//macka//TFM_WD//ORI//SEED_GER//01-EEG-raw//SEED_GER//{emotion}'\n",
    "file_list = [file for file in os.listdir(emotion_folder) if file.endswith('.fif')]\n",
    "\n",
    "# Statistics List\n",
    "means = []\n",
    "covariances = []\n",
    "entropies = []\n",
    "kurtoses = []\n",
    "skewnesses = []\n",
    "standard_deviations = []\n",
    "\n",
    "for file in tqdm(file_list, desc=\"Processing files\", leave=False):\n",
    "    file_path = os.path.join(emotion_folder, file)\n",
    "    eeg_data = mne.io.read_raw_fif(file_path, preload=False)\n",
    "    \n",
    "    data = eeg_data.get_data()\n",
    "    means.append(np.mean(data, axis=1))\n",
    "    covariances.append(np.cov(data))\n",
    "    \n",
    "    def spectral_entropy(data, sfreq):\n",
    "        # Calcular la densidad espectral de potencia usando la función de Welch\n",
    "        freqs, psd = welch(data, sfreq, nperseg=sfreq)\n",
    "        # Normalizar la densidad espectral de potencia\n",
    "        psd_norm = np.divide(psd, np.sum(psd))\n",
    "        # Calcular la entropía espectral (entropía de Shannon)\n",
    "        se = -np.sum(psd_norm * np.log2(psd_norm))\n",
    "        return se\n",
    "\n",
    "    # Calcular la entropía espectral para cada canal\n",
    "    entropies = []\n",
    "    for channel_data in data:\n",
    "        se = spectral_entropy(channel_data, eeg_data.info['sfreq'])\n",
    "        entropies.append(se)\n",
    "\n",
    "    kurtoses.append(scipy.stats.kurtosis(data, axis=1))\n",
    "    skewnesses.append(scipy.stats.skew(data, axis=1))\n",
    "    standard_deviations.append(np.std(data, axis=1))\n",
    "    \n",
    "# Media de las estadisticas de todos los archivos\n",
    "mean_mean = np.mean(means, axis=0)\n",
    "mean_covariance = np.mean(covariances, axis=0)\n",
    "mean_entropy = np.mean(entropies, axis=0)\n",
    "mean_kurtosis = np.mean(kurtoses, axis=0)\n",
    "mean_skewness = np.mean(skewnesses, axis=0)\n",
    "mean_standard_deviation = np.mean(standard_deviations, axis=0)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"{elapsed_time:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cf998d",
   "metadata": {},
   "source": [
    "## PLOT STATISTICS RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea34eca",
   "metadata": {},
   "source": [
    "### AUX FUNCTION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbedbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bargrid(stat_list, stat_name, channel_names):\n",
    "    n_files = len(stat_list)\n",
    "    n_channels = len(channel_names)\n",
    "\n",
    "    fig, axs = plt.subplots(n_files, 1, figsize=(15, n_files * 5), sharex=False)\n",
    "    fig.suptitle(stat_name)\n",
    "\n",
    "    for i, (file_stat, ax) in tqdm(enumerate(zip(stat_list, axs)), desc=\"Generating graphics..\", leave=False):\n",
    "        sns.barplot(x=channel_names, y=file_stat, ax=ax)\n",
    "        ax.set_ylabel(file_list[i])\n",
    "        ax.set_xticklabels(channel_names, rotation=90, fontsize=8)\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "def plot_covariances(cov_list, channel_names):\n",
    "    n_files = len(cov_list)\n",
    "\n",
    "    fig, axs = plt.subplots(n_files, 1, figsize=(15, n_files * 5), sharex=False)\n",
    "    fig.suptitle(\"Covariances\")\n",
    "\n",
    "    for i, (file_cov, ax) in tqdm(enumerate(zip(cov_list, axs)), desc=\"Generating graphics..\", leave=False):\n",
    "        sns.heatmap(file_cov, ax=ax, cmap='coolwarm', xticklabels=channel_names, yticklabels=channel_names)\n",
    "        ax.set_ylabel(file_list[i])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04533b7",
   "metadata": {},
   "source": [
    "###  GRAPHICS CHOOSE: MEAN, CONVARIANCE, SPECTRAL-ENTROPY, SKEWNESSES, STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56372f91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "channel_names = eeg_data.ch_names\n",
    "\n",
    "# Distribución de las listas de estadisticas.\n",
    "\n",
    "# plot_bargrid(means, \"Means\", channel_names) # - MEDIA\n",
    "plot_covariances(covariances, channel_names) # -  COVARIANZA\n",
    "# plot_bargrid(entropies, \"Entropies\", channel_names) # - ENTROPÍA ESPECTRAL\n",
    "# plot_bargrid(kurtoses, \"Kurtoses\", channel_names) # - KURTOSES\n",
    "# plot_bargrid(skewnesses, \"Skewnesses\", channel_names) # - SIMETRÍA\n",
    "# plot_bargrid(standard_deviations, \"Standard Deviations\", channel_names) # - DESV. STANDARD\n",
    "\n",
    "# print_statistics(means, \"Means\")\n",
    "# print_statistics(covariances, \"Covariances\")\n",
    "# print_statistics(entropies, \"Entropies\")\n",
    "# print_statistics(kurtoses, \"Kurtoses\")\n",
    "# print_statistics(skewnesses, \"Skewnesses\")\n",
    "# print_statistics(standard_deviations, \"Standard Deviations\")\n",
    "\n",
    "# def print_statistics(stat_list, stat_name):\n",
    "#     print(f\"{stat_name}:\\n\")\n",
    "#     for i, file_stat in enumerate(stat_list):\n",
    "#         print(f\"Archivo {i + 1}:\")\n",
    "#         for channel_name, channel_stat in zip(channel_names, file_stat):\n",
    "#             print(f\"{channel_name}: {channel_stat}\")\n",
    "#         print()\n",
    "\n",
    "\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"{elapsed_time:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4d3d75",
   "metadata": {},
   "source": [
    "**Media:** La media de los valores de cada canal puede indicar si hay un sesgo en los datos. Si las medias son similares en todos los canales, esto podría sugerir que los datos son consistentes entre los canales. Si un canal tiene una media significativamente diferente, podría ser un indicador de problemas en ese canal, como ruido o artefactos.\n",
    "\n",
    "**Covarianza:** La covarianza mide la relación entre los canales de EEG. Valores altos de covarianza indican que los canales están relacionados, lo que podría sugerir que comparten información o ruido común. Por otro lado, valores bajos indican que los canales son independientes entre sí. La matriz de covarianza también puede ser útil para la reducción de dimensiones y el análisis de componentes principales.\n",
    "\n",
    "**La entropía espectral** es una medida que cuantifica la incertidumbre o complejidad en la distribución de energía de una señal en el dominio de la frecuencia. En el caso de las señales de EEG, la entropía espectral se utiliza para estimar la distribución del espectro de potencia de las señales a lo largo de diferentes bandas de frecuencia.\n",
    "\n",
    "La entropía espectral se calcula utilizando la **entropía de Shannon**, que se basa en la teoría de la información. La entropía de Shannon mide la cantidad de información o incertidumbre en una distribución de probabilidad. En el caso de la entropía espectral, primero se calcula la densidad espectral de potencia (PSD) de la señal de EEG utilizando técnicas como el método de Welch. Luego, se normaliza la PSD para obtener una distribución de probabilidad y, finalmente, se aplica la entropía de Shannon a esta distribución normalizada.\n",
    "\n",
    "Calcular la entropía espectral en el análisis de EEG es importante por varias razones:\n",
    "\n",
    "- **Caracterización de la señal:** La entropía espectral permite describir la complejidad de la señal de EEG en términos de su distribución de energía a lo largo de diferentes frecuencias. Esto puede ser útil para identificar patrones en la actividad cerebral y para comparar diferentes estados cerebrales.\n",
    "\n",
    "- **Detección de cambios en el estado cerebral:** Cambios en la entropía espectral pueden reflejar cambios en la dinámica cerebral, como transiciones entre diferentes estados de conciencia o cambios en la actividad cerebral debido a estímulos externos.\n",
    "\n",
    "- **Extracción de características para el aprendizaje automático:** La entropía espectral puede ser utilizada como una característica en algoritmos de aprendizaje automático para clasificar señales de EEG según diferentes estados o condiciones, como emociones o tareas cognitivas.\n",
    "\n",
    "- **Análisis de la conectividad funcional:** La entropía espectral puede ser utilizada para estudiar la conectividad funcional entre diferentes regiones cerebrales, al comparar la complejidad espectral en diferentes canales de EEG.\n",
    "\n",
    "**Curtosis:** La curtosis mide la \"cola pesada\" o la concentración de valores extremos en una distribución. Valores altos de curtosis indican que hay una mayor concentración de valores extremos, mientras que valores bajos indican una menor concentración. Esto puede ser útil para identificar canales con actividad inusual o ruido impulsivo.\n",
    "\n",
    "**Asimetría (Skewness):** La asimetría mide la falta de simetría en la distribución de los datos. Valores positivos de asimetría indican que hay una cola más larga en el lado derecho de la distribución, mientras que valores negativos indican que hay una cola más larga en el lado izquierdo. La asimetría puede ser útil para identificar distribuciones no simétricas de actividad cerebral que podrían estar relacionadas con procesos cognitivos o emocionales específicos.\n",
    "\n",
    "**Desviación estándar:** La desviación estándar mide la dispersión o variabilidad de los datos. Valores altos de desviación estándar indican que los datos son más dispersos, mientras que valores bajos indican que los datos son más uniformes. La desviación estándar puede ser útil para identificar la consistencia en los datos de EEG entre ensayos y sujetos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac120c4",
   "metadata": {},
   "source": [
    "## ¿Que hacer con los canales o sujetos que se salen de la media? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1727c7",
   "metadata": {},
   "source": [
    "## GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c9b291",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# 1. Cargar y preparar los datos\n",
    "emotion = 'POSITIVE'  # 'NEGATIVE' 'NEUTRAL'\n",
    "emotion_folder = f'C://Users//macka//TFM_WD//ORI//SEED_GER//01-EEG-raw//SEED_GER//{emotion}'\n",
    "file_list = [file for file in os.listdir(emotion_folder) if file.endswith('.fif')]\n",
    "\n",
    "eeg_data_list = []\n",
    "for file in file_list:\n",
    "    file_path = os.path.join(emotion_folder, file)\n",
    "    eeg_data = mne.io.read_raw_fif(file_path, preload=False)\n",
    "    eeg_data_list.append(eeg_data.get_data())\n",
    "\n",
    "# Determine the common length for interpolation\n",
    "common_length = 10000  # You can adjust this value depending on your needs\n",
    "\n",
    "interpolated_eeg_data_list = []\n",
    "\n",
    "for eeg_data in eeg_data_list:\n",
    "    num_channels = eeg_data.shape[0]\n",
    "    original_length = eeg_data.shape[1]\n",
    "    new_eeg_data = np.zeros((num_channels, common_length))\n",
    "\n",
    "    for ch_idx in range(num_channels):\n",
    "        x_original = np.linspace(0, 1, original_length)\n",
    "        x_new = np.linspace(0, 1, common_length)\n",
    "        new_eeg_data[ch_idx] = np.interp(x_new, x_original, eeg_data[ch_idx])\n",
    "\n",
    "    interpolated_eeg_data_list.append(new_eeg_data)\n",
    "\n",
    "eeg_data_tensor = np.stack(interpolated_eeg_data_list, axis=0)\n",
    "eeg_data_tensor = (eeg_data_tensor - np.min(eeg_data_tensor)) / (np.max(eeg_data_tensor) - np.min(eeg_data_tensor)) * 2 - 1\n",
    "\n",
    "# 2. Crear el modelo GAN\n",
    "# Generador\n",
    "# def build_generator(latent_dim):\n",
    "#     input_layer = Input(shape=(latent_dim,))\n",
    "#     x = Dense(128)(input_layer)\n",
    "#     x = LeakyReLU(alpha=0.2)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dense(256)(x)\n",
    "#     x = LeakyReLU(alpha=0.2)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dense(512)(x)\n",
    "#     x = LeakyReLU(alpha=0.2)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dense(np.prod(eeg_data_tensor.shape[1:]), activation='tanh')(x)\n",
    "#     output_layer = Reshape(eeg_data_tensor.shape[1:])(x)\n",
    "\n",
    "#     return Model(input_layer, output_layer)\n",
    "def build_generator(latent_dim):\n",
    "    input_layer = Input(shape=(latent_dim,))\n",
    "    \n",
    "    x = Dense(256)(input_layer)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(512)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(1024)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(2048)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(np.prod(eeg_data_tensor.shape[1:]), activation='tanh')(x)\n",
    "    output_layer = Reshape(eeg_data_tensor.shape[1:])(x)\n",
    "\n",
    "    return Model(input_layer, output_layer)\n",
    "\n",
    "# Discriminador\n",
    "def build_discriminator():\n",
    "    input_layer = Input(shape=eeg_data_tensor.shape[1:])\n",
    "    x = Flatten()(input_layer)\n",
    "    x = Dense(512)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    output_layer = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(input_layer, output_layer)\n",
    "\n",
    "# GAN\n",
    "def build_gan(generator, discriminator):\n",
    "    z = Input(shape=(latent_dim,))\n",
    "    generated_eeg = generator(z)\n",
    "    validity = discriminator(generated_eeg)\n",
    "\n",
    "    return Model(z, validity)\n",
    "\n",
    "latent_dim = 100\n",
    "generator = build_generator(latent_dim)\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "discriminator.trainable = False\n",
    "\n",
    "gan = build_gan(generator, discriminator)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "\n",
    "# 3. Entrenar el GAN\n",
    "# Aquí puedes agregar el código para entrenar el GAN utilizando tus datos de EEG preprocesados (eeg_data_tensor).\n",
    "# Consulta este tutorial para entender cómo entrenar\n",
    "# 3. Entrenar el GAN\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "half_batch = int(batch_size / 2)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Seleccionar un batch aleatorio de EEG reales\n",
    "    idx = np.random.randint(0, eeg_data_tensor.shape[0], half_batch)\n",
    "    real_eegs = eeg_data_tensor[idx]\n",
    "\n",
    "    # Generar un batch de EEG falsos\n",
    "    noise = np.random.normal(0, 1, (half_batch, latent_dim))\n",
    "    fake_eegs = generator.predict(noise)\n",
    "\n",
    "    # Entrenar el discriminador\n",
    "    real_loss = discriminator.train_on_batch(real_eegs, np.ones((half_batch, 1)))\n",
    "    fake_loss = discriminator.train_on_batch(fake_eegs, np.zeros((half_batch, 1)))\n",
    "    d_loss = 0.5 * np.add(real_loss, fake_loss)\n",
    "\n",
    "    # Entrenar el generador\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "    g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "    # Imprimir el progreso\n",
    "    print(f\"Epoch {epoch}/{epochs} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]}] [G loss: {g_loss}]\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"El código tardó {elapsed_time:.2f} segundos en ejecutarse.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc18b54",
   "metadata": {},
   "source": [
    "**D_loss (Discriminator loss):** Es la pérdida del discriminador, que mide qué tan bien el discriminador puede distinguir entre datos reales y datos generados. Un valor bajo de D_loss indica que el discriminador tiene un buen rendimiento en la clasificación de los datos de entrada como reales o falsos. En el ejemplo proporcionado, D_loss es 0.0039080461574485525, lo que sugiere que el discriminador está haciendo un buen trabajo en este punto del entrenamiento.\n",
    "\n",
    "**acc. (Discriminator accuracy):** Es la precisión del discriminador, que mide el porcentaje de datos de entrada que el discriminador clasifica correctamente como reales o falsos. En el ejemplo proporcionado, la precisión del discriminador es del 100%, lo que significa que el discriminador ha clasificado correctamente todos los datos de entrada en esta época.\n",
    "\n",
    "**G_loss (Generator loss):** Es la pérdida del generador, que mide qué tan bien el generador puede engañar al discriminador generando datos falsos que se parezcan a los datos reales. Un valor alto de G_loss indica que el generador necesita mejorar su capacidad para generar datos que parezcan reales. En el ejemplo proporcionado, G_loss es 5.9058027267456055, lo que sugiere que el generador aún puede mejorar en la generación de datos que parezcan reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35d99e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_artificial_eeg(generator, latent_dim):\n",
    "    # Generar ruido aleatorio con la dimensión latente\n",
    "    noise = np.random.normal(0, 1, (1, latent_dim))\n",
    "    \n",
    "    # Usar el generador para crear un EEG artificial a partir del ruido\n",
    "    artificial_eeg = generator.predict(noise)\n",
    "    \n",
    "    return artificial_eeg\n",
    "\n",
    "# Generar un EEG artificial\n",
    "generated_eeg = generate_artificial_eeg(generator, latent_dim)\n",
    "\n",
    "# # Opcional: Convertir el EEG artificial a su rango original de valores\n",
    "# original_range_generated_eeg = (generated_eeg + 1) / 2 * (np.max(eeg_data_tensor) - np.min(eeg_data_tensor)) + np.min(eeg_data_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866de5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_generated_eeg(generated_eeg, original_eeg_data, channel_names, sfreq=250):\n",
    "    # Crear un objeto RawArray de MNE a partir del EEG generado\n",
    "    info = mne.create_info(ch_names=channel_names, sfreq=sfreq, ch_types='eeg')\n",
    "    raw_generated_eeg = mne.io.RawArray(generated_eeg, info)\n",
    "\n",
    "    # Crear un objeto RawArray de MNE a partir del EEG original\n",
    "    random_file = random.choice(file_list)\n",
    "    random_file_path = os.path.join(emotion_folder, random_file)\n",
    "    raw_original_eeg = mne.io.read_raw_fif(random_file_path, preload=True)\n",
    "    eeg_data_array = raw_original_eeg.get_data()\n",
    "    # Visualizar las señales de ambos EEG (original y generado)\n",
    "    print(\"Original EEG:\")\n",
    "    eeg_data_array.plot(duration=12, scalings=\"auto\", show=True)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Generated EEG:\")\n",
    "    raw_generated_eeg.plot(duration=12, scalings=\"auto\", show=True)\n",
    "    plt.show()\n",
    "\n",
    "# Asumiendo que 'eeg_data_list' contiene los datos originales de EEG cargados anteriormente\n",
    "# random_file = random.choice(file_list)\n",
    "# random_file_path = os.path.join(emotion_folder, random_file)\n",
    "# eeg_raw_sample = mne.io.read_raw_fif(random_file_path, preload=False)\n",
    "\n",
    "# Visualizar la señal generada y una señal original de ejemplo\n",
    "plot_generated_eeg(original_range_generated_eeg[0], eeg_data_list[0], channel_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe6d71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cargar y preparar los datos\n",
    "emotion = 'POSITIVE'  # 'NEGATIVE' 'NEUTRAL'\n",
    "emotion_folder = f'C://Users//macka//TFM_WD//ORI//SEED_GER//01-EEG-raw//SEED_GER//{emotion}'\n",
    "file_list = [file for file in os.listdir(emotion_folder) if file.endswith('.fif')]\n",
    "\n",
    "eeg_data_list = []\n",
    "for file in file_list:x\n",
    "    file_path = os.path.join(emotion_folder, file)\n",
    "    eeg_data = mne.io.read_raw_fif(file_path, preload=True)\n",
    "    eeg_data_list.append(eeg_data.get_data())\n",
    "\n",
    "# Determine the common length for interpolation\n",
    "common_length = 3000  # You can adjust this value depending on your needs\n",
    "\n",
    "interpolated_eeg_data_list = []\n",
    "\n",
    "for eeg_data in eeg_data_list:\n",
    "    num_channels = eeg_data.shape[0]\n",
    "    original_length = eeg_data.shape[1]\n",
    "    new_eeg_data = np.zeros((num_channels, common_length))\n",
    "\n",
    "    for ch_idx in range(num_channels):\n",
    "        x_original = np.linspace(0, 1, original_length)\n",
    "        x_new = np.linspace(0, 1, common_length)\n",
    "        new_eeg_data[ch_idx] = np.interp(x_new, x_original, eeg_data[ch_idx])\n",
    "\n",
    "    interpolated_eeg_data_list.append(new_eeg_data)\n",
    "\n",
    "eeg_data_tensor = np.stack(interpolated_eeg_data_list, axis=0)\n",
    "eeg_data_tensor = (eeg_data_tensor - np.min(eeg_data_tensor)) / (np.max(eeg_data_tensor) - np.min(eeg_data_tensor)) * 2 - 1\n",
    "\n",
    "# 2. Crear el modelo GAN\n",
    "# Generador\n",
    "# def build_generator(latent_dim):\n",
    "#     input_layer = Input(shape=(latent_dim,))\n",
    "#     x = Dense(128)(input_layer)\n",
    "#     x = LeakyReLU(alpha=0.2)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dense(256)(x)\n",
    "#     x = LeakyReLU(alpha=0.2)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dense(512)(x)\n",
    "#     x = LeakyReLU(alpha=0.2)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dense(np.prod(eeg_data_tensor.shape[1:]), activation='tanh')(x)\n",
    "#     output_layer = Reshape(eeg_data_tensor.shape[1:])(x)\n",
    "\n",
    "#     return Model(input_layer, output_layer)\n",
    "def build_generator(latent_dim):\n",
    "    input_layer = Input(shape=(latent_dim,))\n",
    "    \n",
    "    x = Dense(256)(input_layer)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(512)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(1024)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(2048)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(np.prod(eeg_data_tensor.shape[1:]), activation='tanh')(x)\n",
    "    output_layer = Reshape(eeg_data_tensor.shape[1:])(x)\n",
    "\n",
    "    return Model(input_layer, output_layer)\n",
    "\n",
    "# Discriminador\n",
    "def build_discriminator():\n",
    "    input_layer = Input(shape=eeg_data_tensor.shape[1:])\n",
    "    x = Flatten()(input_layer)\n",
    "    x = Dense(512)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    output_layer = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(input_layer, output_layer)\n",
    "\n",
    "# GAN\n",
    "def build_gan(generator, discriminator):\n",
    "    z = Input(shape=(latent_dim,))\n",
    "    generated_eeg = generator(z)\n",
    "    validity = discriminator(generated_eeg)\n",
    "\n",
    "    return Model(z, validity)\n",
    "\n",
    "latent_dim = 100\n",
    "generator = build_generator(latent_dim)\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "discriminator.trainable = False\n",
    "\n",
    "gan = build_gan(generator, discriminator)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "\n",
    "# 3. Entrenar el GAN\n",
    "# Aquí puedes agregar el código para entrenar el GAN utilizando tus datos de EEG preprocesados (eeg_data_tensor).\n",
    "# Consulta este tutorial para entender cómo entrenar\n",
    "# 3. Entrenar el GAN\n",
    "epochs = 20000\n",
    "batch_size = 32\n",
    "half_batch = int(batch_size / 2)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Seleccionar un batch aleatorio de EEG reales\n",
    "    idx = np.random.randint(0, eeg_data_tensor.shape[0], half_batch)\n",
    "    real_eegs = eeg_data_tensor[idx]\n",
    "\n",
    "    # Generar un batch de EEG falsos\n",
    "    noise = np.random.normal(0, 1, (half_batch, latent_dim))\n",
    "    fake_eegs = generator.predict(noise)\n",
    "\n",
    "    # Entrenar el discriminador\n",
    "    real_loss = discriminator.train_on_batch(real_eegs, np.ones((half_batch, 1)))\n",
    "    fake_loss = discriminator.train_on_batch(fake_eegs, np.zeros((half_batch, 1)))\n",
    "    d_loss = 0.5 * np.add(real_loss, fake_loss)\n",
    "\n",
    "    # Entrenar el generador\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "    g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "    # Imprimir el progreso\n",
    "    print(f\"Epoch {epoch}/{epochs} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]}] [G loss: {g_loss}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d43f363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
